<html>

<head>
    <title>Motion Pose Detection : Immanuel (Developer)</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="description"
        content="Immanuel developer, Personal website, works, open source contributor, Object Detection" />
    <meta name="keywords"
        content="Immanuel Raj, developer, programmer, c#, javascript, key value pair store, open source contributor, video formatter, motion pose detection" />
    <meta name="robots" content="index, follow" />
    <meta name="distribution" content="Global" />
    <meta name="rating" content="General" />
    <meta name="identifier-url" content="https://www.immanuel.co" />
    <meta name="Language" content="en, en-us, en-gb, en-bz, en-za" />
    <meta name="viewport" content="initial-scale = 1.0,maximum-scale = 1.0" />
    <link rel="shortcut icon" type="image/png" href="https://immanuel.co/favicon.ico" />
    <!-- https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>
    <script src="posenet.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/ark-js-util@1.1.5/ark-util.js"></script>
    <style>
        .footer {
            position: fixed;
            left: 0;
            bottom: 0;
            width: 100%;
            color: black;
        }

        .footer-text {
            max-width: 600px;
            text-align: center;
            margin: auto;
        }

        @media only screen and (max-width: 600px) {

            .footer-text,
            .dg {
                display: none;
            }
        }

        /*
         *  The following loading spinner CSS is from SpinKit project
         *  https://github.com/tobiasahlin/SpinKit
         */
        .sk-spinner-pulse {
            width: 20px;
            height: 20px;
            margin: auto 10px;
            float: left;
            background-color: #333;
            border-radius: 100%;
            -webkit-animation: sk-pulseScaleOut 1s infinite ease-in-out;
            animation: sk-pulseScaleOut 1s infinite ease-in-out;
        }

        @-webkit-keyframes sk-pulseScaleOut {
            0% {
                -webkit-transform: scale(0);
                transform: scale(0);
            }

            100% {
                -webkit-transform: scale(1.0);
                transform: scale(1.0);
                opacity: 0;
            }
        }

        @keyframes sk-pulseScaleOut {
            0% {
                -webkit-transform: scale(0);
                transform: scale(0);
            }

            100% {
                -webkit-transform: scale(1.0);
                transform: scale(1.0);
                opacity: 0;
            }
        }

        .spinner-text {
            float: left;
        }
    </style>
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <div id="info" style='display:none'></div>
    <div id="debug-log">
        <h3>Debug Log</h3>
    </div>
    <video id="video" playsinline style="display: none;">
    </video>
    <canvas id="output" />
    <script>
        var debuglog = document.getElementById("debug-log");
        debuglog.style.display = 'none';
        var qs = util.qs();
        if (qs['debug']) debuglog.style.display = 'block';
        const color = 'aqua';
        const boundingBoxColor = 'red';
        const lineWidth = 2;
        function isMobile() {
            return isAndroid() || isiOS();
        }
        function isAndroid() {
            return /Android/i.test(navigator.userAgent);
        }

        function isiOS() {
            return /iPhone|iPad|iPod/i.test(navigator.userAgent);
        }

        function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {
            const adjacentKeyPoints =
                posenet.getAdjacentKeyPoints(keypoints, minConfidence);

            adjacentKeyPoints.forEach((keypoints) => {
                drawSegment(
                    toTuple(keypoints[0].position), toTuple(keypoints[1].position), color,
                    scale, ctx);
            });
        }

        function drawBoundingBox(keypoints, ctx) {
            const boundingBox = posenet.getBoundingBox(keypoints);

            ctx.rect(
                boundingBox.minX, boundingBox.minY, boundingBox.maxX - boundingBox.minX,
                boundingBox.maxY - boundingBox.minY);

            ctx.strokeStyle = boundingBoxColor;
            ctx.stroke();
        }
        function toTuple({ y, x }) {
            return [y, x];
        }

        async function renderToCanvas(a, ctx) {
            const [height, width] = a.shape;
            const imageData = new ImageData(width, height);

            const data = await a.data();

            for (let i = 0; i < height * width; ++i) {
                const j = i * 4;
                const k = i * 3;

                imageData.data[j + 0] = data[k + 0];
                imageData.data[j + 1] = data[k + 1];
                imageData.data[j + 2] = data[k + 2];
                imageData.data[j + 3] = 255;
            }

            ctx.putImageData(imageData, 0, 0);
        }

        function renderImageToCanvas(image, size, canvas) {
            canvas.width = size[0];
            canvas.height = size[1];
            const ctx = canvas.getContext('2d');

            ctx.drawImage(image, 0, 0);
        }

        function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
            ctx.beginPath();
            ctx.moveTo(ax * scale, ay * scale);
            ctx.lineTo(bx * scale, by * scale);
            ctx.lineWidth = lineWidth;
            ctx.strokeStyle = color;
            ctx.stroke();
        }

        function drawOffsetVectors(
            heatMapValues, offsets, outputStride, scale = 1, ctx) {
            const offsetPoints =
                posenet.singlePose.getOffsetPoints(heatMapValues, outputStride, offsets);

            const heatmapData = heatMapValues.buffer().values;
            const offsetPointsData = offsetPoints.buffer().values;

            for (let i = 0; i < heatmapData.length; i += 2) {
                const heatmapY = heatmapData[i] * outputStride;
                const heatmapX = heatmapData[i + 1] * outputStride;
                const offsetPointY = offsetPointsData[i];
                const offsetPointX = offsetPointsData[i + 1];

                drawSegment(
                    [heatmapY, heatmapX], [offsetPointY, offsetPointX], color, scale, ctx);
            }
        }

        function drawHeatMapValues(heatMapValues, outputStride, canvas) {
            const ctx = canvas.getContext('2d');
            const radius = 5;
            const scaledValues = heatMapValues.mul(tf.scalar(outputStride, 'int32'));

            drawPoints(ctx, scaledValues, radius, color);
        }


        function drawPoint(ctx, y, x, r, color) {
            ctx.beginPath();
            ctx.arc(x, y, r, 0, 2 * Math.PI);
            ctx.fillStyle = color;
            ctx.fill();
        }

        function drawPoints(ctx, points, radius, color) {
            const data = points.buffer().values;

            for (let i = 0; i < data.length; i += 2) {
                const pointY = data[i];
                const pointX = data[i + 1];

                if (pointX !== 0 && pointY !== 0) {
                    ctx.beginPath();
                    ctx.arc(pointX, pointY, radius, 0, 2 * Math.PI);
                    ctx.fillStyle = color;
                    ctx.fill();
                }
            }
        }

        function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {
            for (let i = 0; i < keypoints.length; i++) {
                const keypoint = keypoints[i];

                if (keypoint.score < minConfidence) {
                    continue;
                }

                const { y, x } = keypoint.position;
                drawPoint(ctx, y * scale, x * scale, 3, color);
            }
        }
        const defaultQuantBytes = 2;

        const defaultMobileNetMultiplier = isMobile() ? 0.50 : 0.75;
        const defaultMobileNetStride = 16;
        const defaultMobileNetInputResolution = 500;

        const defaultResNetMultiplier = 1.0;
        const defaultResNetStride = 32;
        const defaultResNetInputResolution = 250;

        const guiState = {
            algorithm: 'multi-pose',
            input: {
                architecture: 'MobileNetV1',
                outputStride: defaultMobileNetStride,
                inputResolution: defaultMobileNetInputResolution,
                multiplier: defaultMobileNetMultiplier,
                quantBytes: defaultQuantBytes
            },
            singlePoseDetection: {
                minPoseConfidence: 0.1,
                minPartConfidence: 0.5,
            },
            multiPoseDetection: {
                maxPoseDetections: 5,
                minPoseConfidence: 0.15,
                minPartConfidence: 0.1,
                nmsRadius: 30.0,
            },
            output: {
                showVideo: true,
                showSkeleton: true,
                showPoints: true,
                showBoundingBox: false,
            },
            net: null,
        };

    </script>
    <script>
        const detectorConfig = {
            modelType: poseDetection.movenet.modelType.MULTIPOSE_LIGHTNING,
            enableTracking: true,
            trackerType: poseDetection.TrackerType.BoundingBox
        };
        async function cc() {
            await tf.ready()
            //var detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, detectorConfig);
            //console.log(detector)
            bindPage();
        }
        cc();

        const videoWidth = 400;
        const videoHeight = 800;

        async function setupCamera() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                throw new Error(
                    'Browser API navigator.mediaDevices.getUserMedia not available');
            }

            const video = document.getElementById('video');
            video.width = videoWidth;
            video.height = videoHeight;

            const mobile = isMobile();
            const stream = await navigator.mediaDevices.getUserMedia({
                'audio': false,
                'video': {
                    facingMode: 'user',
                    width: mobile ? undefined : videoWidth,
                    height: mobile ? undefined : videoHeight,
                },
            });
            video.srcObject = stream;

            return new Promise((resolve) => {
                video.onloadedmetadata = () => {
                    resolve(video);
                };
            });
        }

        async function loadVideo() {
            const video = await setupCamera();
            video.play();

            return video;
        }

        function detectPoseInRealTime(video, net) {
            const canvas = document.getElementById('output');
            const ctx = canvas.getContext('2d');

            // since images are being fed from a webcam, we want to feed in the
            // original image and then just flip the keypoints' x coordinates. If instead
            // we flip the image, then correcting left-right keypoint pairs requires a
            // permutation on all the keypoints.
            const flipPoseHorizontal = true;

            canvas.width = videoWidth;
            canvas.height = videoHeight;

            async function poseDetectionFrame() {


                let poses = [];
                let minPoseConfidence;
                let minPartConfidence;
                switch (guiState.algorithm) {
                    case 'single-pose':
                        const pose = await guiState.net.estimatePoses(video, {
                            flipHorizontal: flipPoseHorizontal,
                            decodingMethod: 'single-person'
                        });
                        poses = poses.concat(pose);
                        minPoseConfidence = +guiState.singlePoseDetection.minPoseConfidence;
                        minPartConfidence = +guiState.singlePoseDetection.minPartConfidence;
                        break;
                    case 'multi-pose':
                        let all_poses = await guiState.net.estimatePoses(video, {
                            flipHorizontal: flipPoseHorizontal,
                            decodingMethod: 'multi-person',
                            maxDetections: guiState.multiPoseDetection.maxPoseDetections,
                            scoreThreshold: guiState.multiPoseDetection.minPartConfidence,
                            nmsRadius: guiState.multiPoseDetection.nmsRadius
                        });

                        poses = poses.concat(all_poses);
                        minPoseConfidence = +guiState.multiPoseDetection.minPoseConfidence;
                        minPartConfidence = +guiState.multiPoseDetection.minPartConfidence;
                        break;
                }

                ctx.clearRect(0, 0, videoWidth, videoHeight);

                if (guiState.output.showVideo) {
                    ctx.save();
                    ctx.scale(-1, 1);
                    ctx.translate(-videoWidth, 0);
                    ctx.drawImage(video, 0, 0, videoWidth, videoHeight);
                    ctx.restore();
                }

                // For each pose (i.e. person) detected in an image, loop through the poses
                // and draw the resulting skeleton and keypoints if over certain confidence
                // scores
                console.log(poses);
                poses.forEach(({ score, keypoints }) => {
                    if (score >= minPoseConfidence) {
                        if (guiState.output.showPoints) {
                            drawKeypoints(keypoints, minPartConfidence, ctx);
                        }
                        if (guiState.output.showSkeleton) {
                            drawSkeleton(keypoints, minPartConfidence, ctx);
                        }
                        if (guiState.output.showBoundingBox) {
                            drawBoundingBox(keypoints, ctx);
                        }
                    }
                });

                // End monitoring code for frames per second
                //stats.end();

                requestAnimationFrame(poseDetectionFrame);
            }

            poseDetectionFrame();
        }

        async function bindPage() {
            const net = await posenet.load({
                architecture: 'MobileNetV1',
                outputStride: 16,
                inputResolution: { width: 414, height: 896 },
                multiplier: 0.75,
                quantBytes: 2
            });
            guiState.net = net;

            let video;

            try {
                video = await loadVideo();
            } catch (e) {
                let info = document.getElementById('info');
                info.textContent = 'this browser does not support video capture,' +
                    'or this device does not have a camera';
                info.style.display = 'block';
                throw e;
            }

            //setupGui([], net);
            //setupFPS();
            detectPoseInRealTime(video, net);
        }

        navigator.getUserMedia = navigator.getUserMedia ||
            navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
        // kick off the demo
        //bindPage();
    </script>
    <script type="text/javascript" src="dat.gui.min.js"></script>
    <script src="https://mrdoob.github.io/stats.js/build/stats.min.js"></script>
</body>

</html>